{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe66c242",
   "metadata": {},
   "source": [
    "##### Yogesh Kumar Gopal\n",
    "##### 8996403\n",
    "##### Assignment-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08867075",
   "metadata": {},
   "source": [
    "##### This code trains a Deep Q-Network (DQN) to play Pong using a CNN that learns from game frames.  \n",
    "##### It preprocesses each frame, stores experience in a replay buffer, and uses sampled batches to update the network.  \n",
    "##### A target network stabilizes learning, while epsilon-greedy controls exploration.  \n",
    "##### The model trains for 20 episodes and plots the score and average rewards to show learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dbf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoges\\AppData\\Local\\Temp\\ipykernel_10216\\2856802192.py:3: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not hasattr(np, \"bool8\"):\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "d:\\Canada\\studies\\Conestoga\\AI Course\\reinforcement\\Assignment3-Reinforcement-Learning-Programming\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# === FIX NUMPY BOOL ISSUE (for Gym compatibility) ===\n",
    "import numpy as np\n",
    "if not hasattr(np, \"bool8\"):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "# === IMPORTS ===\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ================================================================\n",
    "# 1. CNN MODEL FOR DQN\n",
    "# ================================================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape=(4, 84, 80), n_actions=6):\n",
    "        super(DQN, self).__init__()\n",
    "        c, h, w = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(c, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "\n",
    "        self.fc1 = nn.Linear(linear_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ================================================================\n",
    "# 2. MEMORY-OPTIMIZED REPLAY BUFFER\n",
    "# ================================================================\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity=20000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store as uint8 to save memory\n",
    "        self.buffer.append((\n",
    "            state.astype(np.uint8),\n",
    "            action,\n",
    "            reward,\n",
    "            next_state.astype(np.uint8),\n",
    "            done\n",
    "        ))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # convert back to float32 for model input\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ================================================================\n",
    "# 3. FRAME PREPROCESSING\n",
    "# ================================================================\n",
    "def process_frame(img, image_shape):\n",
    "    img = img[30:-12, :, :]             # crop\n",
    "    img = img[::2, ::2]                 # downsample\n",
    "    img = np.mean(img, axis=2).astype(np.uint8)  # grayscale\n",
    "    img = (img - 128) / 128.0           # normalize [-1, 1]\n",
    "    return img.reshape(image_shape)     # (84, 80)\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)\n",
    "\n",
    "# ================================================================\n",
    "# 4. EPSILON-GREEDY POLICY\n",
    "# ================================================================\n",
    "def select_action(model, state, epsilon, n_actions, device):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.tensor(np.array([state]), dtype=torch.float32).to(device)\n",
    "        q_values = model(state_tensor)\n",
    "        return int(torch.argmax(q_values).item())\n",
    "\n",
    "# ================================================================\n",
    "# 5. TRAINING FUNCTION\n",
    "# ================================================================\n",
    "def train_dqn(num_episodes=200, batch_size=8, target_update=10):\n",
    "    ENV_ID = \"PongDeterministic-v4\"\n",
    "    env = gym.make(ENV_ID)\n",
    "    n_actions = env.action_space.n\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    policy_net = DQN().to(DEVICE)\n",
    "    target_net = DQN().to(DEVICE)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "    memory = ReplayMemory(20000)\n",
    "\n",
    "    epsilon, eps_min, eps_decay = 1.0, 0.05, 0.995\n",
    "    gamma = 0.95\n",
    "\n",
    "    scores, avg_rewards = [], []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        frame = process_frame(obs, (84, 80))\n",
    "        state = np.stack([frame] * 4, axis=0)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(policy_net, state, epsilon, n_actions, DEVICE)\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            reward = transform_reward(reward)\n",
    "            next_frame = process_frame(next_obs, (84, 80))\n",
    "            next_state = np.concatenate((state[1:], np.expand_dims(next_frame, 0)), axis=0)\n",
    "\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(memory) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "\n",
    "                states = torch.tensor(states, dtype=torch.float32).to(DEVICE)\n",
    "                actions = torch.tensor(actions).unsqueeze(1).to(DEVICE)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32).to(DEVICE)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "                q_values = policy_net(states).gather(1, actions)\n",
    "                next_q_values = target_net(next_states).max(1)[0].detach().unsqueeze(1)\n",
    "                expected_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, expected_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # decay epsilon\n",
    "        epsilon = max(eps_min, epsilon * eps_decay)\n",
    "        scores.append(total_reward)\n",
    "        avg_reward = np.mean(scores[-5:]) if len(scores) >= 5 else np.mean(scores)\n",
    "        avg_rewards.append(avg_reward)\n",
    "\n",
    "        # live progress every 10 episodes\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {episode+1}/{num_episodes} | Score: {total_reward:.2f} | \"\n",
    "                  f\"Avg(5): {avg_reward:.2f} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(scores, label=\"Score per Episode\")\n",
    "            plt.plot(avg_rewards, label=\"Average Reward (Last 5)\")\n",
    "            plt.legend()\n",
    "            plt.title(\"DQN Training Performance on Pong\")\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Score / Avg Reward\")\n",
    "            plt.show()\n",
    "\n",
    "        # save checkpoint every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            torch.save(policy_net.state_dict(), f\"dqn_pong_ep{episode+1}.pth\")\n",
    "            print(f\"✅ Model checkpoint saved at episode {episode+1}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"Training complete ✅\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(scores, label=\"Score per Episode\")\n",
    "    plt.plot(avg_rewards, label=\"Average Reward (Last 5)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Final DQN Training Performance on Pong (200 Episodes)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score / Avg Reward\")\n",
    "    plt.show()\n",
    "\n",
    "# ================================================================\n",
    "# 6. RUN TRAINING (200 EPISODES)\n",
    "# ================================================================\n",
    "train_dqn(num_episodes=20, batch_size=8, target_update=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb892a",
   "metadata": {},
   "source": [
    "### Summary\n",
    "**Best Config:**  \n",
    "- **Batch:** 8  \n",
    "- **Update:** 10  \n",
    "- **Optimizer:** Adam (lr = 1e-4)  \n",
    "- **Loss:** MSE  \n",
    "\n",
    "**Result:**  \n",
    "Gradual improvement and stable convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6abce1f",
   "metadata": {},
   "source": [
    "###   Talking points about DQN training performance on pong\n",
    "\n",
    "- The blue line goes up and down a lot, which means the computer is still learning and sometimes plays well, but other times not so well.\n",
    "- The orange line is smoother and shows the computer’s average score. It stays low, so the computer is not winning yet.\n",
    "- Even after many tries, both lines stay low, which means the computer still needs more practice to get better at playing Pong.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
